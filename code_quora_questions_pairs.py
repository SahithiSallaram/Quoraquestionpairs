# -*- coding: utf-8 -*-
"""Code_Quora_Questions_Pairs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pZsp9Gld_Hdw_j5snfCP2OcxTB6v19n5

**Installing Libraries**
"""

!pip install Distance
!pip install fuzzywuzzy
!pip install contractions

"""**Importing Libraries**"""

# Commented out IPython magic to ensure Python compatibility.
import re
import os
import spacy
import distance
import contractions
import numpy as np
import pandas as pd
from scipy.sparse import save_npz, load_npz, hstack
from fuzzywuzzy import fuzz
from bs4 import BeautifulSoup
from sklearn.metrics import log_loss, confusion_matrix
from sklearn.calibration import CalibratedClassifierCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
sns.set(style="whitegrid")
colors = sns.color_palette()
import warnings
warnings.filterwarnings('ignore')
from google.colab import drive

"""**Downloading necessary files from nltk**"""

nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')

"""**Mounting Drive to load dataset**"""

drive.mount('/content/drive')
data = pd.read_csv('/content/drive/My Drive/Quora_Questions_data/train.csv')
#displaying the first five rows of data
data.head()

"""**Train and Test split**"""

# Labels array to hold Is_duplicate column
labels = data['is_duplicate'].values
# Removing Is_duplicate column from data
data = data.drop(['is_duplicate'], axis=1)
# Splitting data into 80/20 train and test sets ratio
train_data, test_data, train_labels, test_labels = train_test_split(data, labels, stratify=labels, random_state=42, test_size=0.2)
print('Dimensions in Train data: {}'.format(train_data.shape))
print('Dimensions in test data: {}'.format(test_data.shape))

"""**Preprocessing of Questions**"""

# Returns Processed questions
def preprocess(q):
  # Replacings special characters
  q = q.replace('%', ' percent')
  q = q.replace('$', ' dollar ')
  q = q.replace('₹', ' rupee ')
  q = q.replace('€', ' euro ')
  q = q.replace('@', ' at ')
  # Converting to lowercase and removing spaces
  q = str(q).lower().strip()
  # Replacing  numbers with string equivalents
  q = q.replace(',000,000,000 ', 'b ')
  q = q.replace(',000,000 ', 'm ')
  q = q.replace(',000 ', 'k ')
  q = re.sub(r'([0-9]+)000000000', r'\1b', q)
  q = re.sub(r'([0-9]+)000000', r'\1m', q)
  q = re.sub(r'([0-9]+)000', r'\1k', q)
  # Replacing '[math]' as it is appeared many no. of 900 times in the whole dataset.
  #q = q.replace('[math]', '')
  # expanding words from contractions
  q_expanded_words = []
  for word in q.split():
    # using contractions.fix to expand the shortened words
    q_expanded_words.append(contractions.fix(word))
  q = ' '.join(q_expanded_words)
  # removing other special characters
  pattern = re.compile('\W')
  q = re.sub(pattern, ' ', q).strip()
  # Removing HTML tags
  q = BeautifulSoup(q)
  q = q.get_text()
  return q

# Featurization
def get_token_features(q1, q2):
  div_exp = 0.0001
  # english stop words from nltk
  stop_words = stopwords.words('english')
  # stemming
  stemmer = PorterStemmer()
  token_features = [0.0] * 18
  # splitting questions into tokens
  q1 = q1.split()
  q2 = q2.split()
  # getting common Stop words in both questions
  stops_in_q1 = set([word for word in q1 if word in stop_words])
  stops_in_q2 = set([word for word in q2 if word in stop_words])
  common_stops = stops_in_q1 & stops_in_q2
  # Removing stop words
  q1 = [word for word in q1 if word not in stop_words]
  q2 = [word for word in q2 if word not in stop_words]
  q1_stemmed = ' '.join([word for word in q1])
  q2_stemmed = ' '.join([word for word in q2])
  if len(q1) == 0 or len(q2) == 0:
    return (token_features, q1_stemmed, q2_stemmed)
  # extracting PoS features using nltk
  q1_pos_tagged = nltk.pos_tag(q1)
  q2_pos_tagged = nltk.pos_tag(q2)
  # sets to hold adjectives, nouns, proper nouns
  q1_adj = set()
  q2_adj = set()
  q1_prn = set()
  q2_prn = set()
  q1_n = set()
  q2_n = set()
  for word in q1_pos_tagged:
    if word[1] == 'JJ' or word[1] == 'JJR' or word[1] == 'JJS':
      q1_adj.add(word[0])
    elif word[1] == 'NNP' or word[1] == 'NNPS':
      q1_prn.add(word[0])
    elif word[1] == 'NN' or word[1] == 'NNS':
      q1_n.add(word[0])
  for word in q2_pos_tagged:
    if word[1] == 'JJ' or word[1] == 'JJR' or word[1] == 'JJS':
      q2_adj.add(word[0])
    elif word[1] == 'NNP' or word[1] == 'NNPS':
      q2_prn.add(word[0])
    elif word[1] == 'NN' or word[1] == 'NNS':
      q2_n.add(word[0])
  token_features[15] = len(q1_adj & q2_adj)
  token_features[16] = len(q1_prn & q2_prn)
  token_features[17] = len(q1_n & q2_n)
  # extracting more features
  # last_word_eq
  token_features[13] = int(q1[-1] == q2[-1])
  # first_word_eq
  token_features[14] = int(q1[0] == q2[0])
  # removing duplicate words
  q1 = set(q1)
  q2 = set(q2)
  common_tokens = q1 & q2
  # no. of characters in question 1
  token_features[0] = len(q1_stemmed) * 1.0
  # no. of characters in question 2
  token_features[1] = len(q2_stemmed) * 1.0
  # no. of words in question 1
  token_features[2] = len(q1) * 1.0
  # no. of words in question 2
  token_features[3] = len(q2) * 1.0
  # Sum of words in q1, q2
  token_features[4] = token_features[2] + token_features[3]
  # common words in q1,q2 without repitition
  q1_words = set(q1)
  q2_words = set(q2)
  common_words = q1_words & q2_words
  # no. of common words
  token_features[5] = len(common_words) * 1.0
  # ratio of words_common to words_total
  token_features[6] = token_features[5] / (token_features[4] + div_exp)
  # cwc_min ,cwc_max,csc_min,csc_max,ctc_min,ctc_max
  token_features[7] = token_features[5] / (min(token_features[2], token_features[3]) + div_exp)
  token_features[8] = token_features[5] / (max(token_features[2], token_features[3]) + div_exp)
  token_features[9] = (len(common_stops) * 1.0) / (min(len(stops_in_q1), len(stops_in_q2)) + div_exp)
  token_features[10] = (len(common_stops) * 1.0) / (max(len(stops_in_q1), len(stops_in_q2)) + div_exp)
  token_features[11] = (len(common_tokens) * 1.0) / (min(len(q1), len(q2)) + div_exp)
  token_features[12] = (len(common_tokens) * 1.0) / (max(len(q1), len(q2)) + div_exp) 
  return (token_features, q1_stemmed, q2_stemmed)
# Extracting fuzzy features
def get_fuzzy_features(q1, q2):
  fuzzy_features = [0.0] * 4
  fuzzy_features[0] = fuzz.QRatio(q1, q2)
  fuzzy_features[1] = fuzz.partial_ratio(q1, q2)
  fuzzy_features[2] = fuzz.token_sort_ratio(q1, q2)
  fuzzy_features[3] = fuzz.token_set_ratio(q1, q2)
  return fuzzy_features
# features using length
def get_length_features(q1, q2):
  div_exp = 0.0001
  length_features = [0.0] * 3
  q1_list = q1.strip(' ')
  q2_list = q2.strip(' ')
  # mean_len,abs_len_diff , substring length
  length_features[0] = (len(q1_list) + len(q2_list)) / 2
  length_features[1] = abs(len(q1_list) - len(q2_list))
  substr_len = distance.lcsubstrings(q1, q2, positions=True)[0]
  # calculating longest_substr_ratio
  if substr_len == 0:
    length_features[2] = 0
  else:
    length_features[2] = substr_len / (min(len(q1_list), len(q2_list)) + div_exp)
  return length_features
# Preprocessing and feature extraction
def extract_features(data):
  # preprocessing question1 and question2
  data['question1'] = data['question1'].apply(preprocess)
  data['question2'] = data['question2'].apply(preprocess)
  #getting token features
  token_features = data.apply(lambda x: get_token_features(x['question1'], x['question2']), axis=1)
  q1_stemmed = list(map(lambda x: x[1], token_features))
  q2_stemmed = list(map(lambda x: x[2], token_features))
  token_features = list(map(lambda x: x[0], token_features))
  data['question1'] = q1_stemmed
  data['question2'] = q2_stemmed
  # Adding columns to store extracted features
  data['q1_len'] = list(map(lambda x: x[0], token_features))
  data['q2_len'] = list(map(lambda x: x[1], token_features))
  data['q1_words'] = list(map(lambda x: x[2], token_features))
  data['q2_words'] = list(map(lambda x: x[3], token_features))
  data['words_total'] = list(map(lambda x: x[4], token_features))
  data['words_common'] = list(map(lambda x: x[5], token_features))
  data['words_shared'] = list(map(lambda x: x[6], token_features)) 
  data['cwc_min'] = list(map(lambda x: x[7], token_features))
  data['cwc_max'] = list(map(lambda x: x[8], token_features))
  data['csc_min'] = list(map(lambda x: x[9], token_features))
  data['csc_max'] = list(map(lambda x: x[10], token_features))
  data['ctc_min'] = list(map(lambda x: x[11], token_features))
  data['ctc_max'] = list(map(lambda x: x[12], token_features))
  data['last_word_eq'] = list(map(lambda x: x[13], token_features))
  data['first_word_eq'] = list(map(lambda x: x[14], token_features))
  data['num_common_adj'] = list(map(lambda x: x[15], token_features))
  data['num_common_prn'] = list(map(lambda x: x[16], token_features))
  data['num_common_n'] = list(map(lambda x: x[17], token_features))
  # extracting fuzzy features
  fuzzy_features = data.apply(lambda x: get_fuzzy_features(x['question1'], x['question2']), axis=1)
  data['fuzz_ratio'] = list(map(lambda x: x[0], fuzzy_features))
  data['fuzz_partial_ratio'] = list(map(lambda x: x[1], fuzzy_features))
  data['token_sort_ratio'] = list(map(lambda x: x[2], fuzzy_features))
  data['token_set_ratio'] = list(map(lambda x: x[3], fuzzy_features))
  # getting length features
  length_features = data.apply(lambda x: get_length_features(x['question1'], x['question2']), axis=1)
  # Adding columns to store length features
  data['mean_len'] = list(map(lambda x: x[0], length_features))
  data['abs_len_diff'] = list(map(lambda x: x[1], length_features))
  data['longest_substr_ratio'] = list(map(lambda x: x[2], length_features))
  return data

# storing preprocessing results
if os.path.isfile('/content/drive/My Drive/Quora_Questions_data/cleaned_featurized_train.csv'):
  train_data = pd.read_csv('/content/drive/My Drive/Quora_Questions_data/cleaned_featurized_train.csv')
  train_labels = np.load('/content/drive/My Drive/Quora_Questions_data/train_labels.npy')
  test_data = pd.read_csv('/content/drive/My Drive/Quora_Questions_data/cleaned_featurized_test.csv')
  test_labels = np.load('/content/drive/My Drive/Quora_Questions_data/test_labels.npy')
else:
  train_data = extract_features(train_data)
  train_data.to_csv('/content/drive/My Drive/Quora_Questions_data/cleaned_featurized_train.csv', index=False)
  np.save('/content/drive/My Drive/Quora_Questions_data/train_labels.npy', train_labels)
  test_data = extract_features(test_data)
  test_data.to_csv('/content/drive/My Drive/Quora_Questions_data/cleaned_featurized_test.csv', index=False)
  np.save('/content/drive/My Drive/Quora_Questions_data/test_labels.npy', test_labels)

# Displaying train_data after preprocessing
train_data.head(3)

train_data = train_data.assign(is_duplicate=train_labels)

"""Visualizing features

**Words_shared**
"""

# showing words_shared in box plot
plt.figure(figsize=(10, 10))
sns.boxplot(x=train_labels, y='words_shared', data=train_data)
plt.title('Box Plot - Fraction of Shared Words in Both Questions')
plt.xlabel('Class label')
plt.ylabel('Fraction of shared words')
plt.show()

# showing token_sort_ratio in PDF
plt.figure(figsize=(10, 10))
sns.distplot(train_data[train_data['is_duplicate']==0]['token_sort_ratio'], hist=False, label='0', color=colors[0], kde_kws={'shade': True})
sns.distplot(train_data[train_data['is_duplicate']==1]['token_sort_ratio'], hist=False, label='1', color=colors[3], kde_kws={'shade': True})
plt.title('PDF - Token Sort Ratio')
plt.xlabel('token_sort_ratio')
plt.show()

"""#Vectorization"""

# Dropping some feature columns
train_data = train_data.drop(columns=['ctc_min', 'ctc_max', 'fuzz_ratio'])
test_data = test_data.drop(columns=['ctc_min', 'ctc_max', 'fuzz_ratio'])
train_data['question1'] = train_data['question1'].apply(lambda x: str(x))
train_data['question2'] = train_data['question2'].apply(lambda x: str(x))
test_data['question1'] = test_data['question1'].apply(lambda x: str(x))
test_data['question2'] = test_data['question2'].apply(lambda x: str(x))

# TF-IDF vectorizer
vectorizer = TfidfVectorizer(lowercase=False)
# list to hold all questions
questions = list(list(train_data['question1']) + list(train_data['question2']))
print('Fitting')
vectorizer.fit(questions)
# Vectorizing questions
print('Vectorizing question1')
q1_vecs_tfidf_train = vectorizer.transform(train_data['question1'].values)
q1_vecs_tfidf_test = vectorizer.transform(test_data['question1'].values)
print('Vectorizing question2')
q2_vecs_tfidf_train = vectorizer.transform(train_data['question2'].values)
q2_vecs_tfidf_test = vectorizer.transform(test_data['question2'].values)
print('Converting to DataFrames')
q1_tfidf_train = pd.DataFrame.sparse.from_spmatrix(q1_vecs_tfidf_train)
q2_tfidf_train = pd.DataFrame.sparse.from_spmatrix(q2_vecs_tfidf_train)
q1_tfidf_test = pd.DataFrame.sparse.from_spmatrix(q1_vecs_tfidf_test)
q2_tfidf_test = pd.DataFrame.sparse.from_spmatrix(q2_vecs_tfidf_test)

# Dropping initial columns
train_data = train_data.drop(columns=['question1', 'question2', 'qid1', 'qid2', 'is_duplicate', 'id'])
test_data = test_data.drop(columns=['question1', 'question2', 'qid1', 'qid2', 'id'])

"""**Function to plot confusion matrix**"""

def plot_confusion_matrix(true, pred):
  c = confusion_matrix(true, pred)
  precision = (c.T / c.sum(axis=1)).T
  recall = (c / c.sum(axis=0))
  plt.figure(figsize=(24,6))
  labels = ['0', '1']
  cmap = sns.light_palette('blue')
  # Plotting Normal confusion matrix
  plt.subplot(1, 3, 1)
  sns.heatmap(c, cmap=cmap, annot=True, fmt='d', xticklabels=labels, yticklabels=labels)
  plt.xlabel('Predicted')
  plt.ylabel('Actual')
  plt.title('Confusion Matrix')
  # Plotting precision confusion matrix
  plt.subplot(1, 3, 2)
  sns.heatmap(precision, cmap=cmap, annot=True, fmt='.3f', xticklabels=labels, yticklabels=labels)
  plt.xlabel('Predicted')
  plt.ylabel('Actual')
  plt.title('Precision Matrix')
  # Plotting recall confusion matrix
  plt.subplot(1, 3, 3)
  sns.heatmap(recall, cmap=cmap, annot=True, fmt='.3f', xticklabels=labels, yticklabels=labels)
  plt.xlabel('Predicted')
  plt.ylabel('Actual')
  plt.title('Recall Matrix')
  plt.show()

"""#Model: SGDClassifier with hyperparameters"""

train_labels_copy = train_labels
# Scaling features using standardization
scaler = StandardScaler()
# scaling all columns except the last_word_eq & first_word_eq
scaled = train_data.loc[:, (train_data.columns != 'last_word_eq') & (train_data.columns != 'first_word_eq')]
scaled = scaler.fit_transform(scaled)
# stacking the features from above
train_model = np.column_stack((scaled, train_data['last_word_eq'], train_data['first_word_eq']))
scaled = test_data.loc[:, (test_data.columns != 'last_word_eq') & (test_data.columns != 'first_word_eq')]
scaled = scaler.transform(scaled)
test_model = np.column_stack((scaled, test_data['last_word_eq'], test_data['first_word_eq']))

# Adding word vectors to Model
# Stacking question1 and question1 in a row 
train_model = hstack((train_model, q1_vecs_tfidf_train, q2_vecs_tfidf_train))
test_model = hstack((test_model, q1_vecs_tfidf_test, q2_vecs_tfidf_test))

"""#SGDClassifier with Logistic Regression"""

# List of alphas to use in L1, L2 and elasticnet regularization
alpha = [10**x for x in range(-5, 2)]
# log-loss lists for L1, L2 and ElasticNet
l1_logloss = []
l2_logloss = []
elasticnet_logloss = []
# SGDClassifier with log loss
# L1 regularization with diff alphas
print('\nL1 Regularization')
for a in alpha:
  sgd_clf = SGDClassifier(loss='log', alpha=a, penalty='l1', n_jobs=-1, random_state=42)  
  sgd_clf.fit(train_model, train_labels_copy)
  cal_clf = CalibratedClassifierCV(sgd_clf)  
  cal_clf.fit(train_model, train_labels_copy)
  pred_prob = cal_clf.predict_proba(test_model)
  loss = (log_loss(test_labels, pred_prob))
  l1_logloss.append(loss)
  print('Regularization: {}\talpha = {}\tloss is : {}'.format('L1', a, loss))
# L2 regularization with diff alphas
print('\nL2 Regularization')
for a in alpha:
  sgd_clf = SGDClassifier(loss='log', alpha=a, penalty='l2', n_jobs=-1, random_state=42)  
  sgd_clf.fit(train_model, train_labels_copy)
  cal_clf = CalibratedClassifierCV(sgd_clf)  
  cal_clf.fit(train_model, train_labels_copy)
  pred_prob = cal_clf.predict_proba(test_model)
  # log-loss on predictions
  loss = (log_loss(test_labels, pred_prob))
  l2_logloss.append(loss)
  print('Regularization: {}\talpha = {}\tloss is : {}'.format('L2', a, loss))
# Elasticnet regularization with diff alphas
print('\nElasticNet Regularization')
for a in alpha:
  sgd_clf = SGDClassifier(loss='log', alpha=a, penalty='elasticnet', n_jobs=-1, random_state=42)  
  sgd_clf.fit(train_model, train_labels_copy)
  cal_clf = CalibratedClassifierCV(sgd_clf)  
  cal_clf.fit(train_model, train_labels_copy)
  pred_prob = cal_clf.predict_proba(test_model)
  # log-loss on predictions
  loss = (log_loss(test_labels, pred_prob))
  elasticnet_logloss.append(loss)
  print('Regularization: {}\talpha = {}\tloss is : {}'.format('ElasticNet', a, loss))
print('\n')
# plotting L1,L2,Elasticnet
plt.figure(figsize=(15, 8))
sns.lineplot(x=alpha, y=l1_logloss, label='L1',color= 'green')
sns.lineplot(x=alpha, y=l2_logloss, label='L2',color = 'brown')
sns.lineplot(x=alpha, y=elasticnet_logloss, label='ElasticNet',color = 'purple')
# plotting LogLoss Vs Alpha
plt.title('LogLoss vs Alpha')
plt.xlabel('Alpha')
plt.ylabel('Log-loss')
plt.legend()
plt.show()

# SGDClassifier with best alpha and regularization
best_alpha = alpha[np.argmin(l2_logloss)]
print('Logistic regression')
sgd_clf = SGDClassifier(loss='log', alpha=best_alpha, penalty='l2', n_jobs=-1, random_state=42)
sgd_clf.fit(train_model, train_labels_copy)
cal_clf = CalibratedClassifierCV(sgd_clf)
cal_clf.fit(train_model, train_labels_copy)
predict_train = cal_clf.predict_proba(train_model)
predict_test = cal_clf.predict_proba(test_model)
print('Log loss for best alpha on train data: {}'.format(log_loss(train_labels_copy, predict_train)))
print('Log loss for best alpha on test data: {}'.format(log_loss(test_labels, predict_test)))
print('\n')
# Plotting confusion matrix
predicted = np.argmax(predict_test, axis=1)
plot_confusion_matrix(test_labels, predicted)

"""#SGDClassifier with Linear SVM"""

# List of alphas to use in L1, L2 and elasticnet regularization
alpha = [10**x for x in range(-5, 2)]
# log-loss lists for L1, L2 and ElasticNet
l1_logloss = []
l2_logloss = []
elasticnet_logloss = []
# SGDClassifier with hinge loss
# L1 regularization with diff alphas
print('L1 Regularization')
for a in alpha:
  sgd_clf = SGDClassifier(loss='hinge', alpha=a, penalty='l1', n_jobs=-1, random_state=42)
  sgd_clf.fit(train_model, train_labels)
  cal_clf = CalibratedClassifierCV(sgd_clf)  
  cal_clf.fit(train_model, train_labels)
  pred_prob = cal_clf.predict_proba(test_model)
  # log-loss on predictions
  loss = (log_loss(test_labels, pred_prob))
  l1_logloss.append(loss)
  print('Regularization: {}\talpha = {}\tloss is : {}'.format('L1', a, loss))
# L2 regularization with diff alphas
print('\nL2 Regularization')
for a in alpha:
  sgd_clf = SGDClassifier(loss='hinge', alpha=a, penalty='l2', n_jobs=-1, random_state=42)  
  sgd_clf.fit(train_model, train_labels)
  cal_clf = CalibratedClassifierCV(sgd_clf)  
  cal_clf.fit(train_model, train_labels)
  pred_prob = cal_clf.predict_proba(test_model)
  loss = (log_loss(test_labels, pred_prob))
  l2_logloss.append(loss)
  print('Regularization: {}\talpha = {}\tloss is : {}'.format('L2', a, loss))
# elasticnet regularization with diff alphas
print('\nElasticNet Regularization')
for a in alpha:
  sgd_clf = SGDClassifier(loss='hinge', alpha=a, penalty='elasticnet', n_jobs=-1, random_state=42)  
  sgd_clf.fit(train_model, train_labels)
  cal_clf = CalibratedClassifierCV(sgd_clf)  
  cal_clf.fit(train_model, train_labels)
  pred_prob = cal_clf.predict_proba(test_model)
  # log-loss on predictions
  loss = (log_loss(test_labels, pred_prob))
  elasticnet_logloss.append(loss)
  print('Regularization: {}\talpha = {}\tloss is : {}'.format('ElasticNet', a, loss))
print('\n')
# plotting L1,L2,Elasticnet
plt.figure(figsize=(15, 8))
sns.lineplot(x=alpha, y=l1_logloss, label='L1',color= 'green')
sns.lineplot(x=alpha, y=l2_logloss, label='L2',color= 'brown')
sns.lineplot(x=alpha, y=elasticnet_logloss, label='ElasticNet',color= 'purple')
# Plotting LogLoss Vs Alpha
plt.title('LogLoss vs Alpha')
plt.xlabel('Alpha')
plt.ylabel('Log-loss')
plt.legend()
plt.show()

# SGDClassifier with best alpha and regularization
best_alpha = alpha[np.argmin(l2_logloss)]
print('Linear SVM')
sgd_clf = SGDClassifier(loss='hinge', alpha=best_alpha, penalty='l2', n_jobs=-1, random_state=42)
sgd_clf.fit(train_model, train_labels)
cal_clf = CalibratedClassifierCV(sgd_clf)
cal_clf.fit(train_model, train_labels)
predict_train = cal_clf.predict_proba(train_model)
predict_test = cal_clf.predict_proba(test_model)
print('Log loss for best alpha on train data: {}'.format(log_loss(train_labels, predict_train)))
print('Log loss for best alpha on test data: {}'.format(log_loss(test_labels, predict_test)))
print('\n')
# Plotting confusion matrix
predicted = np.argmax(predict_test, axis=1)
plot_confusion_matrix(test_labels, predicted)